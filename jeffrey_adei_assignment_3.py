# -*- coding: utf-8 -*-
"""Jeffrey_Adei_Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mkDup7I1W4IsSqfoA0Kt9wmx8z_72AqH
"""

!pip install scikeras

!pip install tensorflow==<2.15.0> scikit-learn

!pip install --upgrade tensorflow

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns
from sklearn.impute import SimpleImputer
from keras.optimizers import Adam
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
import numpy as np
from scikeras.wrappers import KerasClassifier

df = pd.read_csv('/content/drive/MyDrive/CustomerChurn_dataset.csv')

df.head()

df.describe()

df = df.drop('customerID', axis =1)
df = df.drop('gender', axis = 1)

df.info()

correlation_matrix = df.corr()

plt.figure(figsize=(12,8))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',fmt='2f')

categorical = df.select_dtypes(include=['object'])
categorical

column_to_convert = 'TotalCharges'
df[column_to_convert] = pd.to_numeric(df[column_to_convert], errors='coerce')
df[column_to_convert] = df[column_to_convert].astype(float)

print(categorical)

numerical = df.select_dtypes(include = ['number'])
categorical = df.select_dtypes(exclude = ['number'])

print("Numerical Columns:", numerical)
print("Categorical Columns:", categorical)

numerical.fillna(numerical.mean(), inplace = True)
print(numerical)

scaler = StandardScaler()
numerical_scaled = scaler.fit_transform(numerical)
numerical = pd.DataFrame(numerical_scaled, columns=numerical.columns)

categorical.fillna(categorical.mode().iloc[0], inplace =True)
print(categorical)

encoders = {}

encoded_df = pd.DataFrame()
for column in categorical:
    label_encoder = LabelEncoder()
    encoded_df[column] = label_encoder.fit_transform(categorical[column])
    encoders[column] = label_encoder

for i in encoders:
  print(i)
  label_mapping = dict(zip(encoders[i].classes_, range(len(label_encoder.classes_))))
  print("Label Mapping:", label_mapping)

print(encoded_df)

values_in_columns = {}


for column in encoded_df.columns:
    values_in_columns[column] = encoded_df[column].unique()

# Display the unique values
for column, values in values_in_columns.items():
    print(f"{column}: {values}")

new_df = pd.concat([numerical, encoded_df], axis=1)
new_df



X = new_df.drop('Churn', axis=1)
y = new_df['Churn']

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X_train, y_train)


feature_importances = rfc.feature_importances_


feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})


feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)


plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()

correlations = new_df.corrwith(new_df['Churn'])

columns_to_drop = correlations[abs(correlations)< 0.05].index

new_df = new_df.drop(columns = columns_to_drop)

print(new_df)

input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

y_pred = model.predict(X_test)

y_pred_binary = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_binary)

auc_score = roc_auc_score(y_test, y_pred)

print('Accuracy', accuracy)
print('AUC Score', auc_score)

def create_model(units=32, optimizer='adam'):

    inputs = Input(shape=(X_train.shape[1],))


    hidden_layer = Dense(units, activation='relu')(inputs)


    outputs = Dense(1, activation='sigmoid')(hidden_layer)


    model = Model(inputs=inputs, outputs=outputs)


    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

model= KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

model.get_params().keys()

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


param_grid = {
    'batch_size': [10],
    'optimizer': ['adam', 'sgd']
}


grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
grid_result = grid_search.fit(StandardScaler().fit_transform(X_train), y_train)

print("Best Parameters: ", grid_result.best_params_)
print("Best Accuracy: {:.2f}%".format(grid_result.best_score_ * 100))

X_Corr=X.copy()

import numpy as np
import tensorflow as tf
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.layers import Dropout
from tensorflow.keras.constraints import MaxNorm
from sklearn.metrics import matthews_corrcoef
from imblearn.metrics import geometric_mean_score
from sklearn.model_selection import cross_val_score
from numpy import mean
from numpy import std

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler

# Split the data into train and test sets while preserving class distribution
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize the RandomOverSampler
oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)

# Apply random oversampling to the training data
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)

# Print the original and resampled class distribution
print("Original class distribution:", np.bincount(y_train))
print("Resampled class distribution:", np.bincount(y_train_resampled))

type(y_train)

from sklearn.metrics import accuracy_score,classification_report
from sklearn import metrics
import pickle

num_classes=1
epochs=50
batch_size=10

def create_model(dropout_rate, weight_constraint,neurons,activation):
  # create modeloptimizer=optimizer
  input_shape = (X_Corr.shape[1],)
  inputs = tf.keras.Input(shape=input_shape)
  input = tf.keras.layers.Dense((28)+neurons, activation=activation)(inputs)
  x= tf.keras.layers.Dropout(dropout_rate)(input)
  second=tf.keras.layers.Dense((12)+neurons, activation=activation)(x)
  x= tf.keras.layers.Dropout(dropout_rate)(second)
  third=tf.keras.layers.Dense((4)+neurons, activation=activation)(x)
  x= tf.keras.layers.Dropout(dropout_rate)(third)
  fourth=tf.keras.layers.Dense((-4)+neurons, activation=activation)(x)
  x= tf.keras.layers.Dropout(dropout_rate)(fourth)
  fifth=tf.keras.layers.Dense((-12)+neurons, activation=activation)(x)
# Add more hidden layers if necessary

# Add output layer with softmax activation
  outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(fifth)

# Create the model
  m = tf.keras.Model(inputs=inputs, outputs=outputs)
  m.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])
  return m

model = KerasClassifier(model=create_model, epochs=epochs, batch_size=batch_size, verbose=0)
dropout_rate = [0.3, 0.5]
weight_constraint = [3.0, 5.0]
neurons = [20]
optimizer = ['SGD', 'Adam', 'RMSProp']
activation = ['relu']
param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint,
                  model__neurons=neurons,model__activation=activation)

outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='accuracy')

# Initialize lists to store outer fold results
outer_scores = []
best_models = []
for train_idx, val_idx in outer_cv.split(X_train_resampled, y_train_resampled):
    X_train_outer, X_val_outer = X_train_resampled.iloc[train_idx], X_train_resampled.iloc[val_idx]
    y_train_outer, y_val_outer = y_train_resampled.iloc[train_idx], y_train_resampled.iloc[val_idx]

    # Perform hyperparameter tuning in the inner loop
    grid_search.fit(X_train_outer, y_train_outer)
    best_model = grid_search.best_estimator_
    best_models.append(best_model)

    # Evaluate the best model on the outer validation set
    y_pred_outer = best_model.predict(X_val_outer)
    accuracy = accuracy_score(y_val_outer, y_pred_outer)
    outer_scores.append(accuracy)

print("Outer CV Scores:", outer_scores)
print("Mean Accuracy:", np.mean(outer_scores))
print("Standard Deviation:", np.std(outer_scores))

# Train the final model on the entire training set with the best hyperparameters
final_best_model_mlp = grid_search.best_estimator_
print("The best estimator:",grid_search.best_estimator_, "\n")
final_best_model_mlp.fit(X_train_resampled, y_train_resampled,epochs=epochs, batch_size=batch_size, verbose=0)

# # Evaluate the final model on the holdout test set
# test_accuracy = final_best_model_mlp.score(X_test, Y_test)
# print("Test Set Accuracy:", test_accuracy)

# Compile the model
# m.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])

# Train the model
# m.fit(X_train_resampled, y_train_resampled,epochs=epochs, batch_size=batch_size, verbose=0)

# Evaluate the model and obtain predicted probabilities
y_pred = final_best_model_mlp.predict(X_test)
fpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test, y_pred)
auc_mlp = round(metrics.roc_auc_score(y_test, y_pred), 4)
print("AUC:",auc_mlp)
y_pred=np.round(final_best_model_mlp.predict(X_test)).ravel()
print("\nCR by library method=\n",
          classification_report(y_test, y_pred))

grid_search.best_params_

def create_mlp(neurons=20, activation='relu', dropout_rate=0.3, weight_constraint=3.0):
    input_shape = (X_Corr.shape[1],)
    inputs = tf.keras.Input(shape=input_shape)
    x = Dense(neurons, activation=activation, kernel_constraint=tf.keras.constraints.MaxNorm(weight_constraint))(inputs)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(1, activation='sigmoid')(x)
    f_model = tf.keras.Model(inputs=inputs, outputs=outputs)
    f_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return f_model


input_size = 10

# Create the MLP model
f_model = create_mlp(neurons=20, activation='relu', dropout_rate=0.3, weight_constraint=3.0)

f_model.fit(X_train_resampled, y_train_resampled,epochs=epochs, batch_size=batch_size, verbose=0)

y_pred = f_model.predict(X_test)
fpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test, y_pred)
auc_mlp = round(metrics.roc_auc_score(y_test, y_pred), 4)
print("AUC:",auc_mlp)
y_pred=np.round(final_best_model_mlp.predict(X_test)).ravel()
print("\nCR by library method=\n",
          classification_report(y_test,y_pred))





from tensorflow.keras.models import save_model
import pickle

# Assuming 'model' is your MLP model
model_filename = 'mlp_model.h5'
f_model.save(model_filename)


# Save the scaler
scaler_filename = 'scaler.pkl'
with open(scaler_filename, 'wb') as scaler_file:
    pickle.dump({'scaler': scaler}, scaler_file)

# Save the encoder
encoder_filename = 'encoder.pkl'
with open(encoder_filename, 'wb') as encoder_file:
    pickle.dump({'encoder': label_encoder},encoder_file)

! pip freeze | grep 'numpy\|pandas\|scikit-learn\|tensorflow' > requirements.txt

